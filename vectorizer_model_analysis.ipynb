{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引用需要的library\n",
    "import numpy as np\n",
    "import helper\n",
    "import joblib\n",
    "import os\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读数据\n",
    "# input\n",
    "# folder_path: 数据目录\n",
    "# vectorizer: 向量化算法\n",
    "def Read_comments_from_file(folder_path, vectorizer):\n",
    "    X = []\n",
    "    y = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for dir in dirs:\n",
    "            if dir == 'neg':\n",
    "                label = 0\n",
    "            elif dir == 'pos':\n",
    "                label = 1\n",
    "            else:\n",
    "                continue\n",
    "            for file in os.listdir(os.path.join(folder_path, dir)):\n",
    "                if file.endswith('.txt'):\n",
    "                    file_path = os.path.join(folder_path, dir, file)\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='gb2312', errors='ignore') as f:\n",
    "                            content = f.read()\n",
    "                            content = content.strip().replace(\" \", \"\").replace('\\n', '').replace('\\t', '')\n",
    "                        \n",
    "                            if content == \"\": \n",
    "                                # print(file_path + \" is empty\")\n",
    "                                continue\n",
    "                            \n",
    "                            # 使用jieba进行分词\n",
    "                            words = ' '.join(jieba.cut(content))\n",
    "                            X.append(words)\n",
    "                            y.append(label)\n",
    "                    except:\n",
    "                        # print(file_path + \" has exception\")\n",
    "                        continue\n",
    "\n",
    "    # 文本向量化\n",
    "    # 返回的类型是scipy.sparse._csr.csr_matrix，是一个稀疏矩阵\n",
    "    X = vectorizer.fit_transform(X)\n",
    "\n",
    "    X = X.toarray()\n",
    "    y=np.array(y)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文本文件中读数据\n",
    "# 读取到的内容\n",
    "# X=评论（string）， y=情感分类（0/1）\n",
    "folder_path = './user_comments'  # 请将此处替换为您的文件夹路径\n",
    "# 使用CountVectorizer进行文本向量化\n",
    "vectorizer = CountVectorizer()\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# ont-hot\n",
    "X, y = Read_comments_from_file(folder_path=folder_path, vectorizer=vectorizer)\n",
    "# X=稀疏矩阵（int），y=情感分类（0/1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5999, 21309), (5999,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集切分\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    用函数的方式创建简单的参数搜索\n",
    "'''\n",
    "# 创建简单的超参数搜索\n",
    "from itertools import product\n",
    "def grid_search_simple(X_train, y_train, X_test, y_test, model, param_grid):\n",
    "    best_score = -1\n",
    "    best_params = None\n",
    "    train_duration = None\n",
    "    \n",
    "    # Generate all combinations of hyperparameters\n",
    "    param_combinations = product(*param_grid.values())\n",
    "    \n",
    "    # Perform grid search\n",
    "    for params in param_combinations:\n",
    "        # Convert params tuple to dictionary\n",
    "        params_dict = dict(zip(param_grid.keys(), params))\n",
    "        \n",
    "        # Set model parameters\n",
    "        model.set_params(**params_dict)\n",
    "        \n",
    "        # Time start\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Time end\n",
    "        end = time.perf_counter() \n",
    "\n",
    "        # Evaluate the model\n",
    "        score = model.score(X_test, y_test)\n",
    "        \n",
    "        # Update best score and best parameters if needed\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params_dict\n",
    "            train_duration = round(end - start, 2)\n",
    "    \n",
    "    return best_params, best_score, train_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    用类的方式创建简单的参数搜索\n",
    "    使用方法：\n",
    "        - .fit 训练过程\n",
    "        - .model_name 模型名称\n",
    "        - .best_score 最佳分数\n",
    "        - .best_params 最佳参数\n",
    "        - .train_duration 训练时间\n",
    "    使用说明：\n",
    "        - 没有定义predict，后期需要可以添加，超参训练，直接用socre活得准确率\n",
    "        - 定义这些方法，方便汇总所有模型最佳分数，画图分析对比\n",
    "'''\n",
    "class grid_search_test(object):\n",
    "\n",
    "    def __init__(self, model, param_grid):\n",
    "\n",
    "        self.model = model\n",
    "        self.model_name = type(model).__name__\n",
    "        self.param_grid = param_grid\n",
    "\n",
    "        self.best_score = -1\n",
    "        self.best_params = None\n",
    "        self.train_duration = None\n",
    "\n",
    "    def fit(self, X_train, y_train, X_test, y_test):\n",
    "\n",
    "        # Generate all combinations of hyperparameters\n",
    "        param_combinations = product(*self.param_grid.values()) \n",
    "        \n",
    "        # Perform grid search\n",
    "        for params in param_combinations:\n",
    "            # Convert params tuple to dictionary\n",
    "            params_dict = dict(zip(self.param_grid.keys(), params))\n",
    "            \n",
    "            # Set model parameters\n",
    "            self.model.set_params(**params_dict)\n",
    "\n",
    "            # Time start\n",
    "            start = time.perf_counter()\n",
    "\n",
    "            # Train the model\n",
    "            self.model.fit(X_train, y_train)\n",
    "            \n",
    "            # Time end\n",
    "            end = time.perf_counter() \n",
    "\n",
    "            # Evaluate the model\n",
    "            score = self.model.score(X_test, y_test)\n",
    "            \n",
    "            # Update best score and best parameters if needed\n",
    "            if score > self.best_score:\n",
    "                self.best_score = score\n",
    "                self.best_params = params_dict\n",
    "                self.train_duration = round(end - start, 2)\n",
    "\n",
    "        return self.best_score, self.best_params, self.train_duration\n",
    "    \n",
    "    def best_score(self):\n",
    "        return self.best_score\n",
    "    \n",
    "    def best_params(self):\n",
    "        return self.best_params\n",
    "    \n",
    "    def train_duration(self):\n",
    "        return self.train_duration\n",
    "    \n",
    "    def model_name(self):\n",
    "        return self.model_name\n",
    "\n",
    "  \n",
    "# # 举例说明用法\n",
    "# # Define the model\n",
    "# model = KNeighborsClassifier()\n",
    "\n",
    "# # Define the hyperparameter grid\n",
    "# param_grid = {\n",
    "#     'n_neighbors': [5],  # 近邻数\n",
    "#     'weights': ['uniform', 'distance'],  # 权重函数\n",
    "# }\n",
    "\n",
    "# # Perform grid search\n",
    "# knn_gs = grid_search_test(model, param_grid)\n",
    "# best_params, best_score, train_duration = knn_gs.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# print(f'{knn_gs.model_name} best_score：{knn_gs.best_score}')\n",
    "# print(f'{knn_gs.model_name} best_parameters：{knn_gs.best_params}')\n",
    "# print(f'{knn_gs.model_name} train_duration：{knn_gs.train_duration}')\n",
    "\n",
    "# # KNeighborsClassifier best_score：0.7766666666666666\n",
    "# # KNeighborsClassifier best_parameters：{'n_neighbors': 5, 'weights': 'distance'}\n",
    "# # KNeighborsClassifier train_duration：0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    用类的方式创建简单的参数搜索\n",
    "    使用方法：\n",
    "        - .fit 训练过程\n",
    "        - .model_name 模型名称\n",
    "        - .best_score 最佳分数\n",
    "        - .best_params 最佳参数\n",
    "        - .train_duration 训练时间\n",
    "    使用说明：\n",
    "        - 没有定义predict，后期需要可以添加，超参训练，直接用socre活得准确率\n",
    "        - 定义这些方法，方便汇总所有模型最佳分数，画图分析对比\n",
    "'''\n",
    "# 举例说明用法\n",
    "# Define the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [5],  # 近邻数\n",
    "    'weights': ['uniform', 'distance'],  # 权重函数\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "knn_gs = grid_search_test(model, param_grid)\n",
    "best_params, best_score, train_duration = knn_gs.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(f'{knn_gs.model_name} best_score：{knn_gs.best_score}')\n",
    "print(f'{knn_gs.model_name} best_parameters：{knn_gs.best_params}')\n",
    "print(f'{knn_gs.model_name} train_duration：{knn_gs.train_duration}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. LogisticRegression\n",
    "- 关注的参数：\n",
    "  - C：正则化强度的倒数。C 值越小，正则化强度越大。默认为 1.0。\n",
    "  - penalty：正则化类型。可以选择 'l1'（L1 正则化）或 'l2'（L2 正则化）。默认为 'l2'。\n",
    "  - max_iter：算法收敛的最大迭代次数。默认为 100。\n",
    "  - solver：用于优化问题的算法。可以选择不同的求解器，如 'liblinear'、'lbfgs'、'newton-cg'、'sag' 和 'saga'。默认为 'lbfgs'。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳参数组合： {'C': 0.10778765841014329, 'penalty': 'l2'}\n",
      "最佳交叉验证得分： 0.8593066444259104\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    随机搜索超参数\n",
    "'''\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# 定义LogisticRegression模型\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# 定义要搜索的参数分布\n",
    "param_dist = {\n",
    "    'penalty': ['l1', 'l2'],  # 正则化类型\n",
    "    'C': uniform(0.1, 10),  # 正则化强度的倒数的均匀分布\n",
    "}\n",
    "\n",
    "# 实例化RandomizedSearchCV对象\n",
    "random_search = RandomizedSearchCV(estimator=log_reg, param_distributions=param_dist, n_iter=10, cv=5, random_state=42)\n",
    "\n",
    "# 使用随机搜索拟合数据\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# 输出最佳参数组合和对应的得分\n",
    "print(\"最佳参数组合：\", random_search.best_params_)\n",
    "print(\"最佳交叉验证得分：\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1}\n",
      "Best score: 0.905\n",
      "Train_duration: 5.77\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    自定义超参数方法搜索\n",
    "'''\n",
    "# Define the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # 正则化强度的倒数\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "best_params, best_score, train_duration = grid_search_simple(X_train, y_train, X_test, y_test, model, param_grid)\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score:\", best_score)\n",
    "print(\"Train_duration:\", train_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression best_score：0.905\n",
      "LogisticRegression best_parameters：{'C': 1}\n",
      "LogisticRegression train_duration：6.02\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    自定义超参数类搜索\n",
    "'''\n",
    "# Define the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # 正则化强度的倒数\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "log_gs = grid_search_test(model, param_grid)\n",
    "best_params, best_score, train_duration = log_gs.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(f'{log_gs.model_name} best_score：{log_gs.best_score}')\n",
    "print(f'{log_gs.model_name} best_parameters：{log_gs.best_params}')\n",
    "print(f'{log_gs.model_name} train_duration：{log_gs.train_duration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. KNeighborsClassifier\n",
    "- 关注的参数：\n",
    "  - n_neighbors：近邻数，即要考虑的最近邻居的数量。默认为 5\n",
    "  - weights：用于预测的权重函数。可以选择 'uniform'（所有邻居的权重相等）、'distance'（权重与距离的倒数成正比）或自定义权重函数。默认为 'uniform'。\n",
    "  - algorithm：用于计算最近邻居的算法。可以选择 'auto'、'ball_tree'、'kd_tree' 或 'brute'。默认为 'auto'，会根据训练数据的数量和维度自动选择适合的算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_neighbors': 11, 'weights': 'distance'}\n",
      "Best score: 0.7816666666666666\n",
      "Train_duration: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],  # 近邻数\n",
    "    'weights': ['uniform', 'distance'],  # 权重函数\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "best_params, best_score, train_duration = grid_search_simple(X_train, y_train, X_test, y_test, model, param_grid)\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score:\", best_score)\n",
    "print(\"Train_duration:\", train_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier best_score：0.7816666666666666\n",
      "KNeighborsClassifier best_parameters：{'n_neighbors': 11, 'weights': 'distance'}\n",
      "KNeighborsClassifier train_duration：0.0\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],  # 近邻数\n",
    "    'weights': ['uniform', 'distance'],  # 权重函数\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "knn_gs = grid_search_test(model, param_grid)\n",
    "best_params, best_score, train_duration = knn_gs.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(f'{knn_gs.model_name} best_score：{knn_gs.best_score}')\n",
    "print(f'{knn_gs.model_name} best_parameters：{knn_gs.best_params}')\n",
    "print(f'{knn_gs.model_name} train_duration：{knn_gs.train_duration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 交叉验证的分数没有遍历分数高，猜测的原因：数据稀疏性比较强，交叉验证分成很多子集，子集数据量比较小，导致训练出来的模型在测试集上的效果不好。\n",
    "- --------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. DecisionTreeClassifier\n",
    "- 关注的参数：\n",
    "  - criterion： 决策树的分裂标准，可选值为\"gini\"和\"entropy\"。\n",
    "  - max_depth： 树的最大深度。控制树的最大深度可以防止过拟合。通常需要根据数据的复杂度来调整这个参数。\n",
    "  - min_samples_split： 节点分裂所需的最小样本数。如果一个节点的样本数少于这个值，则不再分裂。增大这个值可以防止过拟合。\n",
    "  - min_samples_leaf： 叶子节点所需的最小样本数。如果一个叶子节点的样本数少于这个值，则不再分裂。增大这个值可以防止过拟合。\n",
    "  - max_features： 节点分裂时考虑的最大特征数。可以是整数（表示具体的特征数），也可以是浮点数（表示特征数的百分比）。默认为\"None\"，表示考虑所有特征。\n",
    "  - max_leaf_nodes： 树的最大叶子节点数。如果指定了这个参数，则树会在达到指定数量的叶子节点后停止生长。\n",
    "  - random_state： 随机数种子，用于控制随机性，保证结果的可重复性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier best_score：0.8441666666666666\n",
      "DecisionTreeClassifier best_parameters：{'criterion': 'entropy', 'max_depth': 100, 'random_state': 1}\n",
      "DecisionTreeClassifier train_duration：6.2\n"
     ]
    }
   ],
   "source": [
    "# 定义DecisionTreeClassifier模型\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# 定义要搜索的参数范围\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  # 根节点的划分方法\n",
    "    'max_depth': [50, 100],  # 树的最大深度\n",
    "    'random_state': [0, 1, 2, 3, 4],  # 随机数种子\n",
    "}\n",
    "\n",
    "# 实例化grid_search对象，并训练\n",
    "dtc_gs = grid_search_test(model, param_grid)\n",
    "best_params, best_score, train_duration = dtc_gs.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# 输出最佳参数组合和对应的得分\n",
    "print(f'{dtc_gs.model_name} best_score：{dtc_gs.best_score}')\n",
    "print(f'{dtc_gs.model_name} best_parameters：{dtc_gs.best_params}')\n",
    "print(f'{dtc_gs.model_name} train_duration：{dtc_gs.train_duration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- --------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. RandomForestClassifier\n",
    "- 关注的参数：\n",
    "  - n_estimators: 树的个数，增加树的数量通常会提高模型的性能，但也会增加计算成本。可以通过交叉验证来选择合适的数量。  \n",
    "  - criterion: 选择特征时的划分标准，可选值为\"gini\"和\"entropy\"。\n",
    "  - max_depth: 树的最大深度。控制树的最大深度可以防止过拟合。通常需要根据数据的复杂度来调整这个参数。\n",
    "  - min_samples_split: 节点分裂所需的最小样本数。如果一个节点的样本数少于这个值，则不再分裂。增大这个值可以防止过拟合\n",
    "  - min_samples_leaf: 叶子节点所需的最小样本数。如果一个叶子节点的样本数少于这个值，则不再分裂。增大这个值可以防止过拟合\n",
    "  - max_features: 最大特征数，默认为\"auto\"，表示考虑 sqrt(n_features) 个特征。\n",
    "  - max_leaf_nodes: 叶子节点的最大个数。\n",
    "  - min_impurity_decrease: 最小信息增益，增大这个值可以防止过拟合。\n",
    "  - oob_score: 是否使用袋外数据进行验证，默认为False。袋外样本是在bootstrap采样中未被选中的样本，可以用来评估模型的泛化能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier best_score：0.9033333333333333\n",
      "RandomForestClassifier best_parameters：{'criterion': 'gini', 'n_estimators': 200}\n",
      "RandomForestClassifier train_duration：34.36\n"
     ]
    }
   ],
   "source": [
    "# 定义RandomForestClassifier模型\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# 定义要搜索的参数范围\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  # 根节点的划分方法\n",
    "    'n_estimators': [50, 100, 200],  # 树的个数\n",
    "}\n",
    "\n",
    "# 实例化grid_search对象，并训练\n",
    "rfc_gs = grid_search_test(model, param_grid)\n",
    "best_params, best_score, train_duration = rfc_gs.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# 输出最佳参数组合和对应的得分\n",
    "print(f'{rfc_gs.model_name} best_score：{rfc_gs.best_score}')\n",
    "print(f'{rfc_gs.model_name} best_parameters：{rfc_gs.best_params}')\n",
    "print(f'{rfc_gs.model_name} train_duration：{rfc_gs.train_duration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- --------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. SVC\n",
    "- 关注的参数：\n",
    "    - C: 正则化参数，控制错误项的惩罚程度。C值越小，模型对错误项的容忍度越高，决策边界更平滑；C值越大，模型对错误项的容忍度越低，决策边界更严格。默认值为1.0。\n",
    "    - kernel: 分类器使用的核函数，可选值为‘linear’、‘poly’、‘rbf’、‘sigmoid’。默认值为'rbf'。\n",
    "    - degree: 多项式核函数的多项式次数，仅在kernel为'poly'时有效。默认为3.\n",
    "    - gamma: 核函数的系数，影响核函数的宽度。gamma值越大，决策边界越不规则，模型对训练数据的拟合程度越高，可能导致过拟合；gamma值越小，决策边界越平滑，模型泛化能力可能更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.1, kernel=rbf, gamma=0.1, Accuracy: 0.615\n",
      "C=0.1, kernel=rbf, gamma=1, Accuracy: 0.4925\n",
      "C=0.1, kernel=rbf, gamma=10, Accuracy: 0.4925\n",
      "C=0.1, kernel=linear, gamma=0.1, Accuracy: 0.89\n",
      "C=0.1, kernel=linear, gamma=1, Accuracy: 0.89\n",
      "C=0.1, kernel=linear, gamma=10, Accuracy: 0.89\n",
      "C=1, kernel=rbf, gamma=0.1, Accuracy: 0.8216666666666667\n",
      "C=1, kernel=rbf, gamma=1, Accuracy: 0.6975\n",
      "C=1, kernel=rbf, gamma=10, Accuracy: 0.6966666666666667\n",
      "C=1, kernel=linear, gamma=0.1, Accuracy: 0.8866666666666667\n",
      "C=1, kernel=linear, gamma=1, Accuracy: 0.8866666666666667\n",
      "C=1, kernel=linear, gamma=10, Accuracy: 0.8866666666666667\n",
      "C=10, kernel=rbf, gamma=0.1, Accuracy: 0.8275\n",
      "C=10, kernel=rbf, gamma=1, Accuracy: 0.6975\n",
      "C=10, kernel=rbf, gamma=10, Accuracy: 0.6966666666666667\n",
      "C=10, kernel=linear, gamma=0.1, Accuracy: 0.8808333333333334\n",
      "C=10, kernel=linear, gamma=1, Accuracy: 0.8808333333333334\n",
      "C=10, kernel=linear, gamma=10, Accuracy: 0.8808333333333334\n"
     ]
    }
   ],
   "source": [
    "kernel = ['rbf', 'linear']\n",
    "C = [0.1, 1, 10]\n",
    "gamma = [0.1, 1, 10]\n",
    "\n",
    "for i in C:\n",
    "    for j in kernel:\n",
    "        for k in gamma:\n",
    "            model = SVC(C=i, kernel=j, gamma=k, random_state=0)\n",
    "            model.fit(X_train, y_train)\n",
    "            print(f\"C={i}, kernel={j}, gamma={k}, Accuracy: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC best_score：0.89\n",
      "SVC best_parameters：{'kernel': 'linear', 'C': 0.1}\n",
      "SVC train_duration：48.95\n"
     ]
    }
   ],
   "source": [
    "# 定义SVC模型\n",
    "model = SVC()\n",
    "\n",
    "# 定义要搜索的参数范围\n",
    "param_grid = {\n",
    "    'kernel': ['linear'],  \n",
    "    'C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# 实例化grid_search对象，并训练\n",
    "svc_gs = grid_search_test(model, param_grid)\n",
    "best_params, best_score, train_duration = svc_gs.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# 输出最佳参数组合和对应的得分\n",
    "print(f'{svc_gs.model_name} best_score：{svc_gs.best_score}')\n",
    "print(f'{svc_gs.model_name} best_parameters：{svc_gs.best_params}')\n",
    "print(f'{svc_gs.model_name} train_duration：{svc_gs.train_duration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVC模型最佳参数组合：{'kernel': 'linear', 'C': 0.1}\n",
    "- SVC模型最佳测试集准确率：0.89\n",
    "- --------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. AdaBoostClassifier\n",
    "- 关注的参数：\n",
    "  - n_estimators: 弱学习器的数量（即迭代次数）。默认为50。增加n_estimators通常会提高模型的性能，但也会增加训练时间。\n",
    "  - learning_rate: 默认为1。较小的学习率需要更多的弱学习器来达到相同的性能，但可能会提高模型的泛化能力。\n",
    "  - algorithm: 用于计算加权错误率的算法。默认为“SAMME.R”。\n",
    "  - base_estimator: 指定要使用的基础分类器，默认为DecisionTreeClassifier(max_depth=1)\n",
    "  - random_state: 默认为None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 200, 'learning_rate': 1.0}\n",
      "Best score: 0.8541666666666666\n",
      "Train_duration: 250.53\n"
     ]
    }
   ],
   "source": [
    "# 时间比较久，请勿轻易执行\n",
    "# 定义AdaBoostClassifier模型\n",
    "model = AdaBoostClassifier()\n",
    "\n",
    "# 定义要搜索的参数范围\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # 弱学习器数量\n",
    "    'learning_rate': [0.01, 0.1, 1.0],  # 学习率\n",
    "}\n",
    "\n",
    "# 实例化grid_search对象，并训练\n",
    "best_params, best_score, train_duration = grid_search_simple(X_train, y_train, X_test, y_test, model, param_grid)\n",
    "\n",
    "# 输出最佳参数组合和对应的得分\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score:\", best_score)\n",
    "print(\"Train_duration:\", train_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier best_score：0.8541666666666666\n",
      "AdaBoostClassifier best_parameters：{'n_estimators': 200, 'learning_rate': 1.0}\n",
      "AdaBoostClassifier train_duration：261.41\n"
     ]
    }
   ],
   "source": [
    "# 时间 23m 44.7s\n",
    "# 定义AdaBoostClassifier模型\n",
    "model = AdaBoostClassifier()\n",
    "\n",
    "# 定义要搜索的参数范围\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # 弱学习器数量\n",
    "    'learning_rate': [0.01, 0.1, 1.0],  # 学习率\n",
    "}\n",
    "\n",
    "# 实例化grid_search对象，并训练\n",
    "ada_gs = grid_search_test(model, param_grid)\n",
    "best_params, best_score, train_duration = ada_gs.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# 输出最佳参数组合和对应的得分\n",
    "print(f'{ada_gs.model_name} best_score：{ada_gs.best_score}')\n",
    "print(f'{ada_gs.model_name} best_parameters：{ada_gs.best_params}')\n",
    "print(f'{ada_gs.model_name} train_duration：{ada_gs.train_duration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- --------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. GradientBoostingClassifier\n",
    "- 关注的参数：\n",
    "  - n_estimators: 树的个数。默认为100。\n",
    "  - max_depth: 树的最大深度。默认为3。限制树的深度可以控制模型的复杂度，防止过拟合。\n",
    "  - learning_rate: 学习率。默认为0.1。\n",
    "  - loss：用于指定损失函数的类型。可以选择 'deviance'（对数损失函数，用于分类问题）或 'exponential'（指数损失函数，用于概率估计问题）。默认为'deviance'。\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier best_score：0.8691666666666666\n",
      "GradientBoostingClassifier best_parameters：{'n_estimators': 200, 'learning_rate': 1.0}\n",
      "GradientBoostingClassifier train_duration：506.38\n"
     ]
    }
   ],
   "source": [
    "# 时间 42m 14.3s\n",
    "# 定义GradientBoostingClassifier模型\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# 定义要搜索的参数范围\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # 弱学习器数量\n",
    "    'learning_rate': [0.01, 0.1, 1.0],  # 学习率\n",
    "}\n",
    "\n",
    "# 实例化grid_search对象，并训练\n",
    "gbc_gs = grid_search_test(model, param_grid)\n",
    "best_params, best_score, train_duration = gbc_gs.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# 输出最佳参数组合和对应的得分\n",
    "print(f'{gbc_gs.model_name} best_score：{gbc_gs.best_score}')\n",
    "print(f'{gbc_gs.model_name} best_parameters：{gbc_gs.best_params}')\n",
    "print(f'{gbc_gs.model_name} train_duration：{gbc_gs.train_duration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. XGBClassifier\n",
    "- 关注的参数：\n",
    "  - learning_rate：学习率，控制每个树的贡献。较小的学习率可以使模型更加保守，通常需要较多的迭代次数。默认为0.3。\n",
    "  - n_estimators：要构建的树的数量（迭代次数）。默认为100。\n",
    "  - max_depth：每棵树的最大深度。增加深度会增加模型的复杂度，有可能导致过拟合。默认为6。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier best_score：0.8975\n",
      "XGBClassifier best_parameters：{'n_estimators': 200, 'learning_rate': 1.0}\n",
      "XGBClassifier train_duration：5.76\n"
     ]
    }
   ],
   "source": [
    "# 定义XGBClassifier模型\n",
    "model = XGBClassifier()\n",
    "\n",
    "# 定义要搜索的参数范围\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # 弱学习器数量\n",
    "    'learning_rate': [0.01, 0.1, 1.0],  # 学习率\n",
    "}\n",
    "\n",
    "# 实例化grid_search对象，并训练\n",
    "xgb_gs = grid_search_test(model, param_grid)\n",
    "best_params, best_score, train_duration = xgb_gs.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# 输出最佳参数组合和对应的得分\n",
    "print(f'{xgb_gs.model_name} best_score：{xgb_gs.best_score}')\n",
    "print(f'{xgb_gs.model_name} best_parameters：{xgb_gs.best_params}')\n",
    "print(f'{xgb_gs.model_name} train_duration：{xgb_gs.train_duration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. LGBMClassifier\n",
    "- 关注的参数：\n",
    "  - n_estimators：要构建的树的数量（迭代次数）。默认为100。\n",
    "  - learning_rate：学习率，控制每次迭代的步长。较小的学习率使得模型更加保守，但需要更多的迭代次数来收敛。默认为0.1。\n",
    "  - max_depth：每棵树的最大深度。增加深度会增加模型的复杂度，有可能导致过拟合。默认为-1，表示没有限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2409, number of negative: 2390\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007516 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 1307\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007918\n",
      "[LightGBM] [Info] Start training from score 0.007918\n",
      "[LightGBM] [Info] Number of positive: 2409, number of negative: 2390\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 1307\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007918\n",
      "[LightGBM] [Info] Start training from score 0.007918\n",
      "[LightGBM] [Info] Number of positive: 2409, number of negative: 2390\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 1307\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007918\n",
      "[LightGBM] [Info] Start training from score 0.007918\n",
      "[LightGBM] [Info] Number of positive: 2409, number of negative: 2390\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 1307\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007918\n",
      "[LightGBM] [Info] Start training from score 0.007918\n",
      "[LightGBM] [Info] Number of positive: 2409, number of negative: 2390\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 1307\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007918\n",
      "[LightGBM] [Info] Start training from score 0.007918\n",
      "[LightGBM] [Info] Number of positive: 2409, number of negative: 2390\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 1307\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007918\n",
      "[LightGBM] [Info] Start training from score 0.007918\n",
      "[LightGBM] [Info] Number of positive: 2409, number of negative: 2390\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 1307\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007918\n",
      "[LightGBM] [Info] Start training from score 0.007918\n",
      "[LightGBM] [Info] Number of positive: 2409, number of negative: 2390\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 1307\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007918\n",
      "[LightGBM] [Info] Start training from score 0.007918\n",
      "[LightGBM] [Info] Number of positive: 2409, number of negative: 2390\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 1307\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007918\n",
      "[LightGBM] [Info] Start training from score 0.007918\n",
      "LGBMClassifier best_score：0.9025\n",
      "LGBMClassifier best_parameters：{'n_estimators': 200, 'learning_rate': 0.1}\n",
      "LGBMClassifier train_duration：0.51\n"
     ]
    }
   ],
   "source": [
    "# 定义LGBMClassifier模型\n",
    "model = LGBMClassifier()\n",
    "\n",
    "# 定义要搜索的参数范围\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # 弱学习器数量\n",
    "    'learning_rate': [0.01, 0.1, 1.0],  # 学习率\n",
    "}\n",
    "\n",
    "# 实例化grid_search对象，并训练\n",
    "lgb_gs = grid_search_test(model, param_grid)\n",
    "best_params, best_score, train_duration = lgb_gs.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# 输出最佳参数组合和对应的得分\n",
    "print(f'{lgb_gs.model_name} best_score：{lgb_gs.best_score}')\n",
    "print(f'{lgb_gs.model_name} best_parameters：{lgb_gs.best_params}')\n",
    "print(f'{lgb_gs.model_name} train_duration：{lgb_gs.train_duration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画图\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.sans-serif']=['SimHei']\n",
    "matplotlib.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "labels = ['KNeighborsClassifier','LogisticRegression','DecisionTreeClassifier','RandomForestClassifier','SVC','AdaBoostClassifier','GradientBoostingClassifier','XGBClassifier','LGBMClassifier']\n",
    "# labels = ['knn','log','dtc','rfc','svc','ada','gbc','xgb','lgb']\n",
    "best_score = [knn_gs.best_score, log_gs.best_score, dtc_gs.best_score, rfc_gs.best_score, svc_gs.best_score, ada_gs.best_score, gbc_gs.best_score, xgb_gs.best_score, lgb_gs.best_score]\n",
    "train_duration = [knn_gs.train_duration, log_gs.train_duration, dtc_gs.train_duration, rfc_gs.train_duration, svc_gs.train_duration, ada_gs.train_duration, gbc_gs.train_duration, xgb_gs.train_duration, lgb_gs.train_duration] \n",
    "train_time = [t / 60 for t in train_duration] # 将秒转换为分钟\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "rects1 = ax.bar(x - width/2, best_score, width, label='best_score')\n",
    "rects2 = ax.bar(x + width/2, train_time, width, label='train_duration: m')\n",
    "\n",
    "# ax.set_ylim(top=1.5)\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('CountVector Best_score & Train_duration')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation=20)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, fmt='%0.3f', padding=3)\n",
    "ax.bar_label(rects2, fmt='%0.3f', padding=3)\n",
    "\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.savefig('./images/Countvector_best_score_train_duration.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KNeighborsClassifier': [0.7816666666666666, 0.0],\n",
       " 'LogisticRegression': [0.905, 0.10033333333333333],\n",
       " 'DecisionTreeClassifier': [0.8441666666666666, 0.10333333333333333],\n",
       " 'RandomForestClassifier': [0.9033333333333333, 0.5726666666666667],\n",
       " 'SVC': [0.89, 0.8158333333333334],\n",
       " 'AdaBoostClassifier': [0.8541666666666666, 4.356833333333333],\n",
       " 'GradientBoostingClassifier': [0.8691666666666666, 8.439666666666666],\n",
       " 'XGBClassifier': [0.8975, 0.096],\n",
       " 'LGBMClassifier': [0.9025, 0.0085]}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存各模型的最有分数和训练时长，便于画图对比\n",
    "\n",
    "labels = ['KNeighborsClassifier','LogisticRegression','DecisionTreeClassifier','RandomForestClassifier','SVC','AdaBoostClassifier','GradientBoostingClassifier','XGBClassifier','LGBMClassifier']\n",
    "best_score = [knn_gs.best_score, log_gs.best_score, dtc_gs.best_score, rfc_gs.best_score, svc_gs.best_score, ada_gs.best_score, gbc_gs.best_score, xgb_gs.best_score, lgb_gs.best_score]\n",
    "train_duration = [knn_gs.train_duration, log_gs.train_duration, dtc_gs.train_duration, rfc_gs.train_duration, svc_gs.train_duration, ada_gs.train_duration, gbc_gs.train_duration, xgb_gs.train_duration, lgb_gs.train_duration] \n",
    "train_time = [t / 60 for t in train_duration] # 将秒转换为分钟\n",
    "\n",
    "CountVectorizer_dict = {}\n",
    "for i in range(len(labels)):\n",
    "    CountVectorizer_dict[labels[i]] = [best_score[i], train_time[i]]\n",
    "\n",
    "# 保存数据\n",
    "joblib.dump(CountVectorizer_dict, './images/CountVectorizer_dict.pkl')\n",
    "\n",
    "CountVectorizer_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier best_parameters：{'n_neighbors': 11, 'weights': 'distance'}\n",
      "LogisticRegression best_parameters：{'C': 1}\n",
      "DecisionTreeClassifier best_parameters：{'criterion': 'entropy', 'max_depth': 100, 'random_state': 1}\n",
      "SVC best_parameters：{'kernel': 'linear', 'C': 0.1}\n",
      "RandomForestClassifier best_parameters：{'criterion': 'gini', 'n_estimators': 100}\n",
      "AdaBoostClassifier best_parameters：{'n_estimators': 200, 'learning_rate': 1.0}\n",
      "GradientBoostingClassifier best_parameters：{'n_estimators': 200, 'learning_rate': 1.0}\n",
      "XGBClassifier best_parameters：{'n_estimators': 200, 'learning_rate': 1.0}\n",
      "LGBMClassifier best_parameters：{'n_estimators': 200, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# 各模型最优参数\n",
    "\n",
    "print(f'{knn_gs.model_name} best_parameters：{knn_gs.best_params}')\n",
    "print(f'{log_gs.model_name} best_parameters：{log_gs.best_params}')\n",
    "print(f'{dtc_gs.model_name} best_parameters：{dtc_gs.best_params}')\n",
    "print(f'{svc_gs.model_name} best_parameters：{svc_gs.best_params}')\n",
    "print(f'{rfc_gs.model_name} best_parameters：{rfc_gs.best_params}')\n",
    "print(f'{ada_gs.model_name} best_parameters：{ada_gs.best_params}')\n",
    "print(f'{gbc_gs.model_name} best_parameters：{gbc_gs.best_params}')\n",
    "print(f'{xgb_gs.model_name} best_parameters：{xgb_gs.best_params}')\n",
    "print(f'{lgb_gs.model_name} best_parameters：{lgb_gs.best_params}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
