{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引用需要的library\n",
    "import numpy as np\n",
    "import helper\n",
    "import joblib\n",
    "import os\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/d6/mm5k9h3n5_924kj876nnys4w0000gn/T/jieba.cache\n",
      "Loading model cost 0.343 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5999, 21410) (5999,)\n",
      "(5999, 21410) (5999,)\n",
      "(5999, 23431) (5999,)\n"
     ]
    }
   ],
   "source": [
    "# 从文本文件中读数据\n",
    "# 读取到的内容\n",
    "# X=评论（string）， y=情感分类（0/1）\n",
    "X = []\n",
    "y = []\n",
    "folder_path = './user_comments'  # 请将此处替换为您的文件夹路径\n",
    "stop_words_path = './stop_words.txt'\n",
    "# 文本向量化\n",
    "vectorizers = [CountVectorizer(), TfidfVectorizer(), helper.OneHotVectorizer()]\n",
    "# vectorizers = [CountVectorizer()]\n",
    "for vectorizer in vectorizers:\n",
    "    i, j = helper.Read_comments_from_file(folder_path=folder_path, vectorizer=vectorizer, stop_words_path=stop_words_path)\n",
    "    X.append(i)\n",
    "    y.append(j)\n",
    "# X=稀疏矩阵（int），y=情感分类（0/1）\n",
    "# CountVectorizer\n",
    "print(X[0].shape, y[0].shape)\n",
    "# TfidfVectorizer\n",
    "print(X[1].shape, y[1].shape)\n",
    "# OneHotVectorizer\n",
    "print(X[2].shape, y[2].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vectorizer in vectorizers:\n",
    "    joblib.dump(value=vectorizer, filename=\"./models/\"+type(vectorizer).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RandomForestClassifier,Counter)(4799), 10.67, 0.1, 0.8941666666666667\n",
      "(RandomForestClassifier,Tfidf)(4799), 9.9, 0.1, 0.8841666666666667\n",
      "(RandomForestClassifier,OneHot)(4799), 8.51, 0.1, 0.8983333333333333\n",
      "执行结束!!!\n"
     ]
    }
   ],
   "source": [
    "# 产生Dataset对象list，3组数据\n",
    "data_sets = []\n",
    "data_sets.append(helper.DataSet(X=X[0], y=y[0], vectorizer=\"Counter\"))\n",
    "data_sets.append(helper.DataSet(X=X[1], y=y[1], vectorizer=\"Tfidf\"))\n",
    "data_sets.append(helper.DataSet(X=X[2], y=y[2], vectorizer=\"OneHot\"))\n",
    "\n",
    "all_classification_models = helper.Get_test_model()\n",
    "\n",
    "my_classification_models = []\n",
    "for data_set in data_sets:\n",
    "    for model in all_classification_models:\n",
    "        my_classification_models.append(helper.PredictModel(model, data_set))\n",
    "        \n",
    "# 训练 & 预测\n",
    "for model in my_classification_models:\n",
    "    model.fit()\n",
    "    model.predict()\n",
    "    # 打印预测结果\n",
    "    print(f\"({model.model_name},{model.classification_data.vectorizer})({model.X_train_pre.shape[0]}), {model.train_duration}, {model.pred_duration}, {model.get_eval()}\")\n",
    "    model.save()\n",
    "\n",
    "print(\"执行结束!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(KNeighborsClassifier,Counter)(4799), 1.85, 1.5, 0.6783333333333333\n",
      "(LogisticRegression,Counter)(4799), 4.68, 0.53, 0.8441666666666666\n",
      "(DecisionTreeClassifier,Counter)(4799), 7.03, 0.37, 0.8125\n",
      "(SVC,Counter)(4799), 122.86, 28.37, 0.8133333333333334\n",
      "(RandomForestClassifier,Counter)(4799), 14.04, 0.54, 0.88\n",
      "(AdaBoostClassifier,Counter)(4799), 139.94, 4.02, 0.8366666666666667\n",
      "(GradientBoostingClassifier,Counter)(4799), 300.47, 0.49, 0.8516666666666667\n",
      "(XGBClassifier,Counter)(4799), 68.24, 0.11, 0.8616666666666667\n",
      "[LightGBM] [Info] Number of positive: 2384, number of negative: 2415\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4292\n",
      "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 994\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496770 -> initscore=-0.012920\n",
      "[LightGBM] [Info] Start training from score -0.012920\n",
      "(LGBMClassifier,Counter)(4799), 3.99, 0.09, 0.85\n",
      "(KNeighborsClassifier,Tfidf)(4799), 2.8, 1.34, 0.6625\n",
      "(LogisticRegression,Tfidf)(4799), 4.46, 0.7, 0.845\n",
      "(DecisionTreeClassifier,Tfidf)(4799), 8.76, 0.66, 0.815\n",
      "(SVC,Tfidf)(4799), 199.15, 37.26, 0.8058333333333333\n",
      "(RandomForestClassifier,Tfidf)(4799), 15.03, 0.79, 0.8825\n",
      "(AdaBoostClassifier,Tfidf)(4799), 143.52, 3.74, 0.8208333333333333\n",
      "(GradientBoostingClassifier,Tfidf)(4799), 309.04, 0.66, 0.855\n",
      "(XGBClassifier,Tfidf)(4799), 71.35, 0.14, 0.8533333333333334\n",
      "[LightGBM] [Info] Number of positive: 2384, number of negative: 2415\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 26693\n",
      "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 994\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496770 -> initscore=-0.012920\n",
      "[LightGBM] [Info] Start training from score -0.012920\n",
      "(LGBMClassifier,Tfidf)(4799), 5.65, 0.15, 0.8608333333333333\n",
      "(KNeighborsClassifier,OneHot)(4799), 2.13, 1.36, 0.6766666666666666\n",
      "(LogisticRegression,OneHot)(4799), 4.88, 0.45, 0.8575\n",
      "(DecisionTreeClassifier,OneHot)(4799), 6.92, 0.48, 0.8066666666666666\n",
      "(SVC,OneHot)(4799), 134.52, 31.41, 0.84\n",
      "(RandomForestClassifier,OneHot)(4799), 14.17, 0.52, 0.8791666666666667\n",
      "(AdaBoostClassifier,OneHot)(4799), 153.5, 3.76, 0.84\n",
      "(GradientBoostingClassifier,OneHot)(4799), 330.65, 0.44, 0.8541666666666666\n",
      "(XGBClassifier,OneHot)(4799), 76.12, 0.09, 0.8716666666666667\n",
      "[LightGBM] [Info] Number of positive: 2384, number of negative: 2415\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010266 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3585\n",
      "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 1195\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496770 -> initscore=-0.012920\n",
      "[LightGBM] [Info] Start training from score -0.012920\n",
      "(LGBMClassifier,OneHot)(4799), 4.06, 0.09, 0.87\n",
      "执行结束!!!\n"
     ]
    }
   ],
   "source": [
    "# 产生Dataset对象list，3组数据\n",
    "data_sets = []\n",
    "data_sets.append(helper.DataSet(X=X[0], y=y[0], vectorizer=\"Counter\"))\n",
    "data_sets.append(helper.DataSet(X=X[1], y=y[1], vectorizer=\"Tfidf\"))\n",
    "data_sets.append(helper.DataSet(X=X[2], y=y[2], vectorizer=\"OneHot\"))\n",
    "\n",
    "# 9个模型\n",
    "all_classification_models = helper.Make_model_classifier()\n",
    "# 构建预测模型列表\n",
    "my_classification_models = []\n",
    "for data_set in data_sets:\n",
    "    for model in all_classification_models:\n",
    "        my_classification_models.append(helper.PredictModel(model, data_set))\n",
    "\n",
    "\n",
    "# 训练 & 预测\n",
    "for model in my_classification_models:\n",
    "    model.fit()\n",
    "    model.predict()\n",
    "    # 打印预测结果\n",
    "    print(f\"({model.model_name},{model.classification_data.vectorizer})({model.X_train_pre.shape[0]}), {model.train_duration}, {model.pred_duration}, {model.get_eval()}\")\n",
    "    model.save()\n",
    "\n",
    "print(\"执行结束!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 柱状图，能够一眼看出各数据的大小，比较数据之间的差别\n",
    "# 分别打印train_duration, pred_duration, acc图\n",
    "result_data = helper.Result_analysis(my_classification_models)\n",
    "helper.Plot_analysis(result_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. 实现TF-IDF - done\n",
    "# 2. 实现one-hot - done\n",
    "# 3. 对稀疏矩阵的数据预处理：标准化/归一化？PCA降维？\n",
    "# 4. 优化图表显示：显示中文？\n",
    "# 5. 优化调用文本向量化的算法的方式 - done\n",
    "# 6. 过滤掉停用词（传参数给jieba），需要找到合适的停用词列表 - 停用词逻辑已加入\n",
    "# 7. jieba能不能接受专用词列表？\n",
    "# 8. 英文字符串问题，修改去掉空格的方式？- done\n",
    "# 9. 怎么判断是否过拟合？\n",
    "# 10. 怎样优化训练策略和推理策略"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
