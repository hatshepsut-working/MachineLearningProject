{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gh\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 引用需要的library\n",
    "import numpy as np\n",
    "import helper\n",
    "import joblib\n",
    "import os\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\gh\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.332 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5999 5999\n",
      "(5999, 150149) (5999,)\n",
      "(5999, 150149) (5999,)\n",
      "(5999, 23226) (5999,)\n"
     ]
    }
   ],
   "source": [
    "# 从文本文件中读数据\n",
    "# 读取到的内容\n",
    "# X=评论（string）， y=情感分类（0/1）\n",
    "X = []\n",
    "y = []\n",
    "folder_path = './user_comments'  # 请将此处替换为您的文件夹路径\n",
    "stop_words_path = './stop_words.txt'\n",
    "# 文本向量化\n",
    "# vectorizers = [CountVectorizer(ngram_range=(1,1)), TfidfVectorizer(ngram_range=(1,1)), helper.OneHotVectorizer()]\n",
    "vectorizers = [CountVectorizer(ngram_range=(1,2)), TfidfVectorizer(ngram_range=(1,2)), helper.OneHotVectorizer()]\n",
    "# vectorizers = [CountVectorizer(), TfidfVectorizer()]\n",
    "\n",
    "# 读文件\n",
    "comments, labels = helper.Read_comments_from_file(folder_path=folder_path, stop_words_path=stop_words_path)\n",
    "\n",
    "print(len(comments), len(labels))\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    # 文本向量化\n",
    "    # 返回的类型是scipy.sparse._csr.csr_matrix，是一个稀疏矩阵\n",
    "    comments_vector = vectorizer.fit_transform(comments)\n",
    "    X.append(comments_vector.toarray())\n",
    "    \n",
    "labels=np.array(labels)\n",
    "y.append(labels)\n",
    "# 打印完整的稀疏矩阵，需要设置：\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# X=稀疏矩阵（int），y=情感分类（0/1）\n",
    "# CountVectorizer\n",
    "print(X[0].shape, y[0].shape)\n",
    "# TfidfVectorizer\n",
    "print(X[1].shape, y[0].shape)\n",
    "# OneHotVectorizer\n",
    "print(X[2].shape, y[0].shape)\n",
    "\n",
    "# 打印词典\n",
    "# voca = vectorizers[0].vocabulary_\n",
    "# print(len(voca))\n",
    "# for i in voca:\n",
    "#     print(i, voca[i])\n",
    "# vocabulary dict\n",
    "\n",
    "# 对词典dict重新排序，按照value的顺序打印dict\n",
    "# voca=sorted(voca.items(), key=lambda x: x[1])\n",
    "# for item in voca:\n",
    "#     print(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存vectorizer到本地\n",
    "for vectorizer in vectorizers:\n",
    "    joblib.dump(value=vectorizer, filename=\"./models/\"+type(vectorizer).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogisticRegression(C=10)]\n",
      "(LogisticRegression,Counter)(4799), 43.87, 0.33, 0.9083333333333333\n",
      "(LogisticRegression,Tfidf)(4799), 40.08, 0.26, 0.9116666666666666\n",
      "(LogisticRegression,OneHot)(4799), 6.87, 0.04, 0.9025\n",
      "执行结束!!!\n"
     ]
    }
   ],
   "source": [
    "# 用于针对某一种模型的调优训练，执行这个地方\n",
    "# 产生Dataset对象list，3组数据\n",
    "data_sets = []\n",
    "data_sets.append(helper.DataSet(X=X[0], y=y[0], vectorizer=\"Counter\"))\n",
    "data_sets.append(helper.DataSet(X=X[1], y=y[0], vectorizer=\"Tfidf\"))\n",
    "data_sets.append(helper.DataSet(X=X[2], y=y[0], vectorizer=\"OneHot\"))\n",
    "\n",
    "all_classification_models = helper.Get_test_model()\n",
    "# all_classification_models = [LGBMClassifier(n_estimators=200, learning_rate=0.1)]\n",
    "print(all_classification_models)\n",
    "\n",
    "my_classification_models = []\n",
    "for data_set in data_sets:\n",
    "    for model in all_classification_models:\n",
    "        my_classification_models.append(helper.PredictModel(model, data_set))\n",
    "        \n",
    "# 训练 & 预测\n",
    "for model in my_classification_models:\n",
    "    model.fit()\n",
    "    model.predict()\n",
    "    # 打印预测结果\n",
    "    print(f\"({model.model_name},{model.classification_data.vectorizer})({model.X_train_pre.shape[0]}), {model.train_duration}, {model.pred_duration}, {model.get_eval()}\")\n",
    "    model.save()\n",
    "\n",
    "print(\"执行结束!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(KNeighborsClassifier,Counter)(4799), 0.0, 6.55, 0.7725\n",
      "(LogisticRegression,Counter)(4799), 43.6, 0.33, 0.9083333333333333\n",
      "(DecisionTreeClassifier,Counter)(4799), 40.21, 0.38, 0.83\n",
      "(SVC,Counter)(4799), 323.86, 78.09, 0.8833333333333333\n",
      "(RandomForestClassifier,Counter)(4799), 93.99, 0.65, 0.9058333333333334\n",
      "(AdaBoostClassifier,Counter)(4799), 1871.06, 64.52, 0.8516666666666667\n",
      "(GradientBoostingClassifier,Counter)(4799), 3560.96, 0.56, 0.8675\n",
      "(XGBClassifier,Counter)(4799), 133.02, 0.38, 0.8841666666666667\n",
      "(LGBMClassifier,Counter)(4799), 3.86, 0.32, 0.9016666666666666\n",
      "(KNeighborsClassifier,Tfidf)(4799), 0.85, 3.11, 0.8566666666666667\n",
      "(LogisticRegression,Tfidf)(4799), 40.93, 0.27, 0.9116666666666666\n",
      "(DecisionTreeClassifier,Tfidf)(4799), 41.59, 0.4, 0.8391666666666666\n",
      "(SVC,Tfidf)(4799), 569.67, 117.55, 0.9058333333333334\n",
      "(RandomForestClassifier,Tfidf)(4799), 88.05, 0.5, 0.9058333333333334\n",
      "(AdaBoostClassifier,Tfidf)(4799), 1877.0, 65.32, 0.8516666666666667\n",
      "(GradientBoostingClassifier,Tfidf)(4799), 3658.1, 0.61, 0.8525\n",
      "(XGBClassifier,Tfidf)(4799), 159.86, 0.46, 0.8716666666666667\n",
      "(LGBMClassifier,Tfidf)(4799), 3.59, 0.16, 0.8958333333333334\n",
      "(KNeighborsClassifier,OneHot)(4799), 0.0, 1.02, 0.7416666666666667\n",
      "(LogisticRegression,OneHot)(4799), 6.96, 0.04, 0.9025\n",
      "(DecisionTreeClassifier,OneHot)(4799), 5.47, 0.05, 0.8491666666666666\n",
      "(SVC,OneHot)(4799), 48.6, 10.75, 0.8825\n",
      "(RandomForestClassifier,OneHot)(4799), 14.62, 0.11, 0.8966666666666666\n",
      "(AdaBoostClassifier,OneHot)(4799), 273.51, 9.55, 0.8591666666666666\n",
      "(GradientBoostingClassifier,OneHot)(4799), 552.71, 0.09, 0.8825\n",
      "(XGBClassifier,OneHot)(4799), 8.97, 0.08, 0.8933333333333333\n",
      "(LGBMClassifier,OneHot)(4799), 1.04, 0.24, 0.8991666666666667\n",
      "执行结束!!!\n"
     ]
    }
   ],
   "source": [
    "# 用于对比所有模型的指标，执行这个地方\n",
    "# 产生Dataset对象list，3组数据\n",
    "data_sets = []\n",
    "data_sets.append(helper.DataSet(X=X[0], y=y[0], vectorizer=\"Counter\"))\n",
    "data_sets.append(helper.DataSet(X=X[1], y=y[0], vectorizer=\"Tfidf\"))\n",
    "data_sets.append(helper.DataSet(X=X[2], y=y[0], vectorizer=\"OneHot\"))\n",
    "\n",
    "# 9个模型\n",
    "all_classification_models = helper.Make_model_classifier()\n",
    "# 构建预测模型列表\n",
    "my_classification_models = []\n",
    "for data_set in data_sets:\n",
    "    for model in all_classification_models:\n",
    "        my_classification_models.append(helper.PredictModel(model, data_set))\n",
    "\n",
    "\n",
    "# 训练 & 预测\n",
    "for model in my_classification_models:\n",
    "    model.fit()\n",
    "    model.predict()\n",
    "    # 打印预测结果\n",
    "    print(f\"({model.model_name},{model.classification_data.vectorizer})({model.X_train_pre.shape[0]}), {model.train_duration}, {model.pred_duration}, {model.get_eval()}\")\n",
    "    model.save()\n",
    "\n",
    "print(\"执行结束!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里用来画图\n",
    "# 柱状图，能够一眼看出各数据的大小，比较数据之间的差别\n",
    "# 分别打印train_duration, pred_duration, acc图\n",
    "result_data = helper.Result_analysis(my_classification_models)\n",
    "helper.Plot_analysis(result_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# PCA降维？\n",
    "# 怎么判断是否过拟合？\n",
    "# 怎样优化训练策略和推理策略\n",
    "\n",
    "# 1. 实现TF-IDF - done\n",
    "# 2. 实现one-hot - done\n",
    "# 3. 对稀疏矩阵的数据预处理：标准化/归一化？- done\n",
    "# 4. 优化图表显示：显示中文？- no need\n",
    "# 5. 优化调用文本向量化的算法的方式 - done\n",
    "# 6. 过滤掉停用词（传参数给jieba），需要找到合适的停用词列表 - done\n",
    "# 7. jieba能不能接受专用词列表？\n",
    "# 8. 英文字符串问题，修改去掉空格的方式？- done\n",
    "# 9. 读文件优化 - done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
